# Q-Learning-Combat-Agent
Practicing Neuro-Networkks 
# Q-Learning Combat Agent

## Overview
This project implements a Q-learning agent using a feed-forward neural network to play a combat scenario using the SEPIA framework. It includes custom neural network architecture, reward function, and feature representation.

## Setup
1. **Download and Unzip**: Download the hw3 package from Piazza and unzip it.
2. **Directories**: The package contains three main directories: `src`, `data`, and `lib`.

### Moving Files to the Right Place
1. **Source Code**:
   - Create a new package called `hw3` in your workspace (e.g., in Eclipse, IntelliJ, etc.).
   - Place the two source files in this package.
   - You will be working on `QAgent.java`. The provided `BadCombatAgent.java` will serve as an opponent.

2. **Data**:
   - Copy the `data/hw3` directory to the `data` folder in your workspace.

3. **Library**:
   - The `lib` directory contains a file `hw3.jar`.
   - Add `hw3.jar` to your `lib` directory and include it in your build path (similar to how `sepia.jar` was added).

### Modifying the .xml Files
- Optionally, modify the `CombatConfig.xml` files to change arguments given to your `QAgent`.

### Run Configurations
- Create a run configuration to execute `CombatConfig.xml`.
- Name it `QAgentvsBadAgent`.

## Goal
The goal is to implement a Q-learning agent with a neural network. The main tasks include:
1. Designing the network architecture.
2. Defining the reward function.
3. Creating a feature representation of the game state.

## Key Files
- `src/hw3/QAgent.java`: Implements the Q-learning agent.
- `src/hw3/BadCombatAgent.java`: Opponent agent for testing.

## Implementation Details
### Neural Network Architecture (20 points)
- Decide on the network architecture, including the number of layers, units per hidden layer, and activation functions.
- The output layer should produce a scalar Q-value.
- Consider using activation functions like Sigmoid or Tanh and scale the output appropriately.

### Reward Function (40 points)
- Define how units are rewarded or punished based on their actions.
- The performance of the agent is highly sensitive to the reward function.

### Feature Representation (40 points)
- Represent the game state in a way that captures relevant information for the Q-function.
- Focus on features relevant to the attacking unit and target unit, such as the number of teammates attacking the target or the number of enemy units left.

## Grading
- The submission will be graded on code quality, algorithm correctness, and the effectiveness of the reward function and feature representation.
- Full points require demonstrating that the agent has learned to fight, shown through a learning curve generated by playing enough games.
- Solutions that compile and are submitted on time will participate in the extra credit tournament.

## Running the Project
1. Set up the run configuration as described.
2. Test the agent by running the configuration.
3. Adjust the network architecture, reward function, and feature representation to optimize performance.

## Contact
For any questions, feel free to reach out!

Good luck!
